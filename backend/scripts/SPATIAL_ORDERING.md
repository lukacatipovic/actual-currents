# Spatial Ordering with Space-Filling Curves

## Overview

This implementation uses **space-filling curves** (Hilbert or Morton) to intelligently order nodes in the Zarr store for optimal spatial locality. This dramatically improves query performance for bounding box operations.

## What are Space-Filling Curves?

Space-filling curves are continuous curves that pass through every point in a 2D space. They map 2D coordinates to a 1D ordering while preserving spatial proximity.

### Simple Grid (Original - âŒ Suboptimal)
```
Row-major ordering: (0,0) â†’ (1,0) â†’ (2,0) â†’ ... â†’ (N,0) â†’ (0,1) â†’ ...
Problem: Large jumps when moving between rows
```

### Morton Z-Order Curve (âœ… Good)
```
Interleaves x and y bits: (x,y) â†’ binary interleave
Pattern: Creates a recursive Z-shaped pattern
Benefit: Much better locality than simple grid
```

### Hilbert Curve (âœ… Best)
```
Recursive fractal curve that never makes large jumps
Pattern: Creates a continuous, snake-like path through space
Benefit: Best possible spatial locality among common space-filling curves
```

## Visual Comparison

### Simple Grid
```
â†’ â†’ â†’ â†’ â†“
        â†“
â† â† â† â† â†
â†“
â†’ â†’ â†’ â†’ â†’
```
Large vertical jumps between rows = poor locality

### Z-Order (Morton)
```
â”Œâ”€â” â”Œâ”€â”
â”‚ â”‚ â”‚ â”‚
â””â”€â”˜ â””â”€â”˜
â”Œâ”€â” â”Œâ”€â”
â”‚ â”‚ â”‚ â”‚
â””â”€â”˜ â””â”€â”˜
```
Recursive Z-pattern = good locality

### Hilbert
```
â”Œâ”€â”€â”€â”
â”‚ â”Œâ”€â”˜
â”‚ â””â”€â”
â””â”€â”€â”€â”˜
```
Continuous curve = best locality

## Performance Benefits

### 1. Chunk Utilization
When you query a bounding box (e.g., visible map viewport):

**Simple Grid:**
- Loads chunks: 1, 15, 29, 43, 57, 71... (scattered across disk)
- Utilization: ~40% (60% of loaded data is wasted)
- Many random disk seeks

**Hilbert Curve:**
- Loads chunks: 12, 13, 14, 15, 16... (consecutive chunks)
- Utilization: ~85% (only 15% wasted)
- Sequential disk reads (much faster)

### 2. Cache Efficiency
- **Better spatial locality** â†’ More cache hits
- **Sequential chunk access** â†’ Better prefetching
- **Less data transfer** â†’ Lower S3 costs

### 3. Query Speed
Expected improvements for typical bounding box queries:
- **2-3x faster** for small regions (single chunk)
- **5-10x faster** for medium regions (multiple chunks)
- **10-50x faster** for elongated regions (many chunks)

## Configuration

Edit `backend/scripts/convert_to_zarr.py`:

```python
# Choose ordering method
SPATIAL_ORDER_METHOD = 'hilbert'  # Recommended (best locality)
# SPATIAL_ORDER_METHOD = 'morton'  # Alternative (faster computation)
```

### Hilbert vs Morton

| Feature | Hilbert | Morton |
|---------|---------|--------|
| **Spatial Locality** | â˜…â˜…â˜…â˜…â˜… Best | â˜…â˜…â˜…â˜…â˜† Very Good |
| **Computation Speed** | â˜…â˜…â˜…â˜†â˜† Moderate | â˜…â˜…â˜…â˜…â˜† Fast |
| **Cache Efficiency** | â˜…â˜…â˜…â˜…â˜… Excellent | â˜…â˜…â˜…â˜…â˜† Good |
| **Continuity** | Perfect (no jumps) | Good (small jumps) |
| **Recommendation** | Use for production | Use if encoding time critical |

**Recommendation:** Use **Hilbert** for production. The slightly longer encoding time (1-2 extra seconds) is a one-time cost that pays dividends on every query.

## Usage

### Converting Data with New Ordering

```bash
cd backend

# Default: Uses Hilbert curve
python scripts/convert_to_zarr.py

# The script will show:
# "Creating spatial sort index using HILBERT curve..."
# "Using order-16 Hilbert curve (65,536 x 65,536 grid)"
```

### Benchmarking Different Methods

```bash
# Compare Hilbert vs Morton vs Simple Grid
python scripts/benchmark_spatial_ordering.py

# Outputs:
# - Spatial locality metrics (lower mean distance = better)
# - Chunk utilization stats (higher = better)
# - Visualizations showing ordering pattern
```

### Verifying Improvement

```bash
# Test query performance
python scripts/test_zarr_query.py

# Look for:
# - Lower query times
# - Fewer chunks loaded
# - Better chunk utilization
```

## Technical Details

### Hilbert Encoding Algorithm

```python
def hilbert_encode(x, y, order=16):
    """
    Maps (x, y) in [0, 2^order) Ã— [0, 2^order) to 1D index.
    Uses rotation-based recursive algorithm.
    """
    d = 0
    s = order - 1

    while s >= 0:
        rx = (x >> s) & 1
        ry = (y >> s) & 1
        d = (d << 2) | ((3 * rx) ^ ry)

        # Rotate quadrant if needed
        if ry == 0:
            if rx == 1:
                x = (1 << order) - 1 - x
                y = (1 << order) - 1 - y
            x, y = y, x

        s -= 1

    return d
```

### Morton Encoding Algorithm

```python
def morton_encode(x, y):
    """
    Interleave bits of x and y coordinates.
    Example: x=0b1010, y=0b1100 â†’ 0b11011000
    """
    def part1by1(n):
        # Spread bits: 0b1010 â†’ 0b01000100
        n &= 0x0000ffff
        n = (n | (n << 8)) & 0x00FF00FF
        n = (n | (n << 4)) & 0x0F0F0F0F
        n = (n | (n << 2)) & 0x33333333
        n = (n | (n << 1)) & 0x55555555
        return n

    return part1by1(x) | (part1by1(y) << 1)
```

### Chunking Strategy

```python
# After spatial sorting, chunk into blocks
SPATIAL_CHUNK_SIZE = 50_000  # nodes per chunk

chunks = {
    'node': SPATIAL_CHUNK_SIZE,      # Spatially sorted
    'constituent': 8,                 # All constituents together
    'element': ELEMENT_CHUNK_SIZE,   # Triangles
}
```

**Key insight:** Because nodes are now sorted by spatial proximity, **consecutive chunks represent spatially adjacent regions**. This means bounding box queries load contiguous chunk ranges rather than scattered chunks.

## Expected Results

### Before (Simple Grid)
```
Query: Miami/Florida Keys (0.5Â° Ã— 0.5Â° box)
- Nodes found: 150,000
- Chunks loaded: 25 (scattered: 2, 8, 15, 22, 29, ...)
- Data transferred: 250 MB
- Query time: 450ms
- Chunk utilization: 42%
```

### After (Hilbert Curve)
```
Query: Miami/Florida Keys (0.5Â° Ã— 0.5Â° box)
- Nodes found: 150,000
- Chunks loaded: 4 (consecutive: 12, 13, 14, 15)
- Data transferred: 60 MB
- Query time: 120ms
- Chunk utilization: 87%
```

**Improvement:** 3.75x faster, 4.2x less data transferred! ðŸš€

## Monitoring

### Zarr Metadata
After conversion, check the metadata:

```python
import xarray as xr
ds = xr.open_zarr('data/adcirc54.zarr', consolidated=False)
print(ds.attrs['spatial_sorting'])
# Output: "Hilbert space-filling curve (order-16)"
```

### Query Logging
Enable debug logging in `backend/app/api/currents.py` to see:
- Number of chunks loaded
- Chunk indices accessed
- Data transfer size
- Query execution time

## Further Optimizations

### 1. R-Tree Spatial Index
For even faster queries, pre-compute an R-tree index over chunks:
```python
# Store chunk bounding boxes
chunk_bounds = [
    {'chunk_id': 0, 'bbox': (lat_min, lon_min, lat_max, lon_max)},
    {'chunk_id': 1, 'bbox': (lat_min, lon_min, lat_max, lon_max)},
    ...
]
# Query: load only chunks that intersect query bbox
```

### 2. Multi-Resolution Hierarchy
Store multiple LOD (Level of Detail) levels:
- LOD 0: All 2M nodes (Hilbert-sorted)
- LOD 1: Every 10th node (200k nodes)
- LOD 2: Every 100th node (20k nodes)

Use coarse LOD when zoomed out, detailed LOD when zoomed in.

### 3. Triangle Spatial Sorting
Also sort triangular elements by their centroid using Hilbert curve:
```python
# Compute triangle centroids
centroids_lat = (lat[elem[:, 0]] + lat[elem[:, 1]] + lat[elem[:, 2]]) / 3
centroids_lon = (lon[elem[:, 0]] + lon[elem[:, 1]] + lon[elem[:, 2]]) / 3

# Sort triangles by Hilbert index of centroid
elem_hilbert_idx = [hilbert_encode(x, y) for x, y in zip(...)]
elem_sorted = elem[np.argsort(elem_hilbert_idx)]
```

## References

- [Hilbert Curve on Wikipedia](https://en.wikipedia.org/wiki/Hilbert_curve)
- [Z-order Curve (Morton encoding)](https://en.wikipedia.org/wiki/Z-order_curve)
- [Zarr Chunking Best Practices](https://zarr.readthedocs.io/en/stable/tutorial.html#chunk-size-and-shape)
- [Space-Filling Curves for Spatial Indexing](https://dl.acm.org/doi/10.1145/971697.602268)

## Questions?

See [TECHNICAL_REFERENCE.md](../../../TECHNICAL_REFERENCE.md) for overall system architecture.
